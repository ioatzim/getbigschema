# getbigschema
Creation of dbml files from given databases and spreadsheets.
Upload dbmls to API

## Requirements
Requires Python 3.6 or above. ???

### Main features
The main features provided by getbigschema are:
* Generating dbml files, from existing databases and spreadsheets
* Generating dbml files, from existing manifest.json and catalog.json files that are produced with dbt methods
* Uploading dbml files to API
* Requesting dbml files from API

### Usage
Code can be executed in any Python interpreter/terminal or framework, including Jupyter, Conda, Pycharm, etc

### Examples


#### Example1: Turn DBT output files to DBML
DBT (data build tool) enables analytics engineers to transform data in their warehouses by simply writing select statements. dbt handles turning these select statements into tables and views. Details of these tables and views are included in dbt-output files manifest.json and catalog.json. Getbigschema can read these files and produce dbml reports. The dbml file looks like below:

###### dbml_file example
```
table "source.demo_dbt.events.generic_metrics_master" [gridX: 0] {
   "FEATURE_ID" NUMBER
   "NAME" TEXT
   Note: "BASE TABLE"
}

//ref_parents: []
//ref_children: ['model.demo_dbt.users']

table "model.demo_dbt.users" [gridX: 1] {
   "ACCOUNT_ID" NUMBER
   "NAME" TEXT
   "ACCOUNT_STATUS_ID" NUMBER
   "TAX_IDENTIFIER" TEXT
   "CREATED_AT" TIMESTAMP_NTZ
   Note: "VIEW"
}

//ref_parents: ['source.demo_dbt.events.generic_metrics_master']
//ref_children: []
```

Output also includes topological_sort.json, a file that includes all tables, with their gridX order. This order represents the stage that the table was created at, for example it is 0 for source tables, 1 for tables (or views) created based on source ones, 2 for tables (or views) created based on tables of order 0 and 1 etc. The      general rule is that each table has an order equal with the max order of its creators, plus 1  

```
{"source.demo_dbt.events.generic_metrics_master": 0, "model.demo_dbt.users": 1}
```

full code is included in file ``` dbt_to_dbml.py ```

#### Example2: Turn SQL schemas and individual sheets to DBML
getbigschema can turn SQL schemas to dbml files. Most databases are supported. User must enter the database details in schema_to_dbml.py file in the folollowing field:

```
databases = [
{'name': 'database_1', 'url': 'sqlite:///test.db'}
]
```

It also supports google spreadsheets. User must enter the google spreadsheet url in the following field:

```
google_sheets = [
{'name': 'Hospital Bed Utilization', 'url': "https://docs.google.com/spreadsheets/d/1I1S9WW4OEHaNpqebM0L5sumoElrmVBCXCTVItHNZdbs/edit?usp=sharing"}
]
```

Output includes one dbml file for each database and one additional dbml file for all the spreadsheets. 

full code is included in file ``` schem_to_dbml.py ```

#### Example3: Upload DBML files to bigschema.io app
We can upload dbml files to bigschema.io app, executing the file dbml_to_api.py
All we need is the API url and a folder with all dbml files to upload
When upload is completed, api returns a unique id per file.
All ids are save in the file "response_ids.json", in key-value pairs, as below:

```
{
"file_1.dmbl": "6192b2c21c2a512293fea622",
"file_2.dmbl": "6172a2c21f2b3a17793fqa342"
}
```

full code is included in file ``` dbml_to_api.py ```

#### Example4: Retreive DBML files from bigschema.io app
Using the API url and the unique id of the file form previous step, we can retreive the file from the API with the following 2 commands:

```
res = requests.get(f'{url}/{id}')
file = json.loads(res.json()['contents'])
```
full code is included in file ``` dbml_from_api.py ```

### Code of Conduct
All contributors are expected to follow the PyPA Code of Conduct.

----------------------------------------------


# Files:
1. schema_extract.py
   User must add databases (name and url) and google spreadsheets (name and url) manually, to databases = [...] and spreadsheets = [...] code lines (see more instructions in      comments inside the files)
   On execution, code creates one dbml file per database and one dbml file for all the spreadsheets.
   
2. parser.py
   Accepts one manifest.json and one catalog.json file as input (these files are created with dbt process)
   On execution, code creates one dbml file and one file named topological_sort.json which includes all tables, with their gridX order. This order represents the stage that        the table is created, eg it's 0 for source tables,1 for tables (or views) created based on source ones, 2 for tables (or views) created based on tables of order 1 etc. The      general rule is that each table has an order greater by 1, than the table (of all tables used for its creation) with max order 

3. dbml_post_to_api.py
   User must manually add a folder-path that contains dbml files. On execution, code uploads all dbml files from this folder to the API and creates a json file/dictionary that    contains filenames as keys and ids as values (eg "demo_full.dmbl": "6192b2c21c2a512293fea123"). An id can be used to get the associated file from the API, with the file        get_dbmlf_from_api.py
   
4. get_dmbl_from_api.py
   User must manually add the id of the dbml file (found in the 'response_ids.json' file generated from dbml_post_to_api.py). On execution, code returns the dbml file from the    api and assigns it to a variable called dbml_file
   
